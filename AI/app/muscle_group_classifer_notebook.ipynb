{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing all the important libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mediapipe_handler import MediaPipeHandler\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "1. Loading Training and Testing Datasets\n",
    "2. Add Label string values\n",
    "3. Looking at workout distributions\n",
    "4. Preprocessing\n",
    "5. Feature Engineering\n",
    "6. Train Random Forest with hyperparameters [with and without feature engineering]\n",
    "7. Train Neural Network with hyperparameters [with and without feature engineering]\n",
    "8. Train SVM with hyperparameters [with and without feature engineering]\n",
    "9. Compare model performance in terms of SPEED,ACCURACY,PERCISION,RECALL,F1-SCORE\n",
    "10. Do the same process but with SMOTE to handle imbalance classes, then compare accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Loading Training and Testing Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediapipe_model = MediaPipeHandler()\n",
    "\n",
    "training_dataset=mediapipe_model.read_csv_to_pd(os.getcwd() + \"/../data/train_new.csv\")[:40000]\n",
    "testing_dataset=mediapipe_model.read_csv_to_pd(os.getcwd() + \"/../data/test_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['left_ankle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Looking at Workout Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Workout_labels=training_dataset['muscle group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_class_distribution(classes):    \n",
    "    plt.figure(figsize=(20,8))\n",
    "    value_counts = classes.value_counts()\n",
    "    percentages = value_counts / value_counts.sum() * 100\n",
    "\n",
    "    # Plot\n",
    "    ax = percentages.plot(kind='bar', color='skyblue')\n",
    "\n",
    "    # Add percentage text on each bar\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.1f}%', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "    plt.title('Training Dataset muscle group Distribution (Percentage)')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.show()\n",
    "display_class_distribution(training_dataset['muscle group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have to make sure that we get right of Unamed 0: column, as it just contains numerical index not very useful, might lead to overfitting if it is left**\n",
    "- Get rid of Unnamed 0:\n",
    "- Get rid of image\n",
    "- Get rid of muscle group\n",
    "- split each feature into x,y,z components, then remove that original feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Removes original feature and splits it into x,y,z components\n",
    "\n",
    "\"\"\"\n",
    "def Preprocess_data(dataframe,columns_to_flatten):\n",
    "    final_df=dataframe.copy()\n",
    "    # Expanding each column into 3 separate columns (x, y, z) and appending it to the final dataframe.\n",
    "    for column in columns_to_flatten:\n",
    "        # print(np.vstack(dataframe[column]).astype(float))\n",
    "        expanded_df=pd.DataFrame(np.vstack(dataframe[column]).astype(float), \n",
    "                           columns=[column+'_x', column+'_y', column+'_z'],\n",
    "                           index=dataframe.index)\n",
    "        new_df = pd.concat([dataframe.drop(column, axis=1), expanded_df], axis=1)\n",
    "        for new_column in new_df.columns:\n",
    "            final_df[new_column] = new_df[new_column]\n",
    "\n",
    "    return final_df.drop(columns=columns_to_flatten,axis=1)\n",
    "\n",
    "\"\"\"\n",
    "Splits dataset into X_train,y_train or X_test,y_test, if you give it training dataset then X_train and y_train\n",
    "\n",
    "\"\"\"\n",
    "def Return_X_y(dataframe,columns_to_delete):\n",
    "    X=dataframe.drop(columns=columns_to_delete)\n",
    "    y=dataframe['muscle group']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_split=['left_shoulder',\n",
    "       'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist',\n",
    "       'right_wrist', 'left_hip', 'right_hip', 'left_knee',\n",
    "       'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "training_dataset_preprocessed=Preprocess_data(training_dataset,features_to_split)\n",
    "X_train, y_train = Return_X_y(training_dataset_preprocessed,['label','muscle group','image','Unnamed: 0'])\n",
    "\n",
    "\n",
    "testing_dataset_preprocessed=Preprocess_data(testing_dataset,features_to_split)\n",
    "X_test, y_test = Return_X_y(testing_dataset_preprocessed,['label','muscle group','image','Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "X_test, y_test = smote.fit_resample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_class_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train Shape\",X_train.shape)\n",
    "\n",
    "print(\"y_train Shape\",y_train.shape)\n",
    "\n",
    "print(\"X_test Shape\",X_test.shape)\n",
    "\n",
    "print(\"y_test Shape\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**66 Features is all the features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(24, 15))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', \n",
    "            vmax=1.0, vmin=0, center=0.5,\n",
    "            square=True, linewidths=.5, annot=True).set(title='Correlation Matrix for all features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features that are highly correlated , one of them should be removed**\n",
    "- it is better to remove the one that is the least correlated with y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.8\n",
    "columns_to_drop = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > correlation_threshold:\n",
    "            columns_to_drop.append(corr_matrix.columns[j])\n",
    "            print(f\"High Correlation between {corr_matrix.columns[i]} and {corr_matrix.columns[j]} --> {corr_matrix.iloc[i, j]:.2f}\")\n",
    "\n",
    "columns_to_drop = list(set(columns_to_drop))\n",
    "print(f\"Columns to drop: {columns_to_drop}\")\n",
    "print(f\"number of columns to drop: {len(columns_to_drop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Columns to drop: {columns_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**53 Columns to drop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature_eng=X_train.drop(columns=columns_to_drop)\n",
    "X_test_feature_eng=X_test.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature_eng.columns\n",
    "# y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes both training_dataset\n",
    "then it will show the result for each one of the models\n",
    "\n",
    "1-Accuracy\n",
    "2-Classification Report\n",
    "3-Confusion Matrix\n",
    "4-Precision, Recall, F1-Score\n",
    "5-Time Taken to train\n",
    "6-Features used to build the model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train_model(model,param_grid,X_train,y_train,X_test,y_test):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,  # 5-fold cross-validation\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=2,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    y_predictions=grid_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_predictions)\n",
    "    report = classification_report(y_test,y_predictions)\n",
    "    confusion_matrix_values = confusion_matrix(y_test,y_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix_values, annot=True, fmt='d', cmap='Blues', cbar=True, \n",
    "                xticklabels=Workout_labels,\n",
    "                yticklabels=Workout_labels)\n",
    "\n",
    "    # Add labels, title, and axis ticks\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix Heatmap')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Accuracy:\", (accuracy*100),\"%\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [20],\n",
    "    # 'min_samples_split': [2, 5, 10],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest with no feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tree_model = RandomForestClassifier(random_state=42)\n",
    "rfc_model = train_model(random_tree_model,param_grid,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/rfc_muscle_group_classifier.pkl\", 'w+') as f:\n",
    "    print(type(rfc_model))\n",
    "    pickle.dump(rfc_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/rfc_muscle_group_classifier.pkl\", 'rb') as f:\n",
    "    rfc_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest with feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_feat_eng_model = train_model(random_tree_model,param_grid,X_train_feature_eng,y_train,X_test_feature_eng,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/rfc_feat_eng_muscle_group_classifier.pkl\", 'w+') as f:\n",
    "    print(type(rfc_feat_eng_model))\n",
    "    pickle.dump(rfc_feat_eng_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/rfc_feat_eng_muscle_group_classifier.pkl\", 'rb') as f:\n",
    "    rfc_feat_eng_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilayer Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,)],  \n",
    "    # 'activation': ['relu', 'tanh'],  \n",
    "    # 'solver': ['adam', 'sgd'],  \n",
    "    # 'alpha': [0.0001, 0.001, 0.01],  \n",
    "    # 'learning_rate': ['constant', 'adaptive'],  \n",
    "    # 'max_iter': [200, 500, 1000]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilayer Perceptron with no feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "mlp_model = train_model(mlp_model,param_grid,X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/mlp_muscle_group_classifier.pkl\", 'w+') as f:\n",
    "    print(type(mlp_model))\n",
    "    pickle.dump(mlp_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/mlp_muscle_group_classifier.pkl\", 'rb') as f:\n",
    "    mlp_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilayer Perceptron with feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_feat_eng_model = MLPClassifier(random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_feature_eng)\n",
    "X_test_scaled = scaler.fit_transform(X_test_feature_eng)\n",
    "\n",
    "mlp_feat_eng_model = train_model(mlp_model,param_grid,X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/mlp_feat_eng_muscle_group_classifier.pkl\", 'w+') as f:\n",
    "    print(type(mlp_feat_eng_model))\n",
    "    pickle.dump(mlp_feat_eng_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/mlp_feat_eng_muscle_group_classifier.pkl\", 'rb') as f:\n",
    "    mlp_feat_eng_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  \n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'gamma': ['scale', 'auto'],  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "svc_model = train_model(svc_model,param_grid,X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/svc_muscle_group_classifier.pkl\", 'w+') as f:\n",
    "    print(type(svc_model))\n",
    "    pickle.dump(svc_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/svc_muscle_group_classifier.pkl\", 'rb') as f:\n",
    "    svc_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_feat_eng_model = SVC(random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_feature_eng)\n",
    "X_test_scaled = scaler.fit_transform(X_test_feature_eng)\n",
    "\n",
    "svc_feat_eng_model = train_model(svc_model,param_grid,X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/svc_feat_eng_muscle_group_classifier.pkl\", 'w+') as f:\n",
    "    print(type(svc_feat_eng_model))\n",
    "    pickle.dump(svc_feat_eng_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + \"/models/svc_feat_eng_muscle_group_classifier.pkl\", 'rb') as f:\n",
    "    svc_feat_eng_model = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
